# MVP-Echo Scribe: Sherpa-ONNX ASR + Pyannote Diarization (hybrid GPU pipeline)
#
# Multi-stage build:
#   Stage 1: Build frontend (Node 20)
#   Stage 2: CUDA runtime + Sherpa-ONNX ASR + PyTorch/Pyannote diarization
#
# Pipeline: Silero VAD → Batch GPU ASR (Sherpa) + Pyannote Diarization → Merge
# Sherpa handles ASR via ONNX Runtime; Pyannote batches speaker embeddings on PyTorch.
#
# Image size: ~12-15GB (Sherpa + PyTorch + Pyannote)
# VRAM usage: ~4-6GB (vs NeMo's ~12-17GB)
# Performance: 18min audio in ~12-18s (ASR ~5-7s + diarization ~5s)

# ── Stage 1: Frontend build ──
FROM node:20-slim AS frontend-build

WORKDIR /build
COPY frontend/package.json frontend/package-lock.json* ./
RUN npm ci
COPY frontend/ ./
RUN npm run build

# ── Stage 2: CUDA runtime + Sherpa-ONNX ASR + Pyannote diarization ──
FROM nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Model directory (populated by entrypoint-sherpa.sh from GitHub releases)
ENV MODEL_DIR=/models/sherpa-onnx

# System dependencies (libgomp1 required by sherpa-onnx C++ binaries)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    ffmpeg \
    libsndfile1 \
    curl \
    bzip2 \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# ── Python dependencies: sherpa-onnx GPU + web stack ──
# Install sherpa-onnx with CUDA 12 + cuDNN 9 support (matches base image)
# GPU wheel includes ONNX Runtime CUDA provider — no separate C++ binary needed
# Wheel index: https://k2-fsa.github.io/sherpa/onnx/cuda.html
RUN pip3 install --no-cache-dir \
    sherpa-onnx==1.12.23+cuda12.cudnn9 -f https://k2-fsa.github.io/sherpa/onnx/cuda.html \
    && pip3 install --no-cache-dir \
    soundfile \
    numpy

# ── PyTorch + Pyannote for GPU-accelerated speaker diarization ──
# PyTorch with CUDA 12.6 support (Pyannote batches all speaker embeddings as GPU tensors)
RUN pip3 install --no-cache-dir \
    torch --index-url https://download.pytorch.org/whl/cu126

# Pin huggingface_hub to avoid deprecated kwarg issue with pyannote-audio 3.3.2
# (pyannote passes deprecated kwarg removed in huggingface_hub >= 0.24)
RUN pip3 install --no-cache-dir "huggingface_hub<0.24"

# Install pyannote.audio (pin to 3.3.2 for compatibility, --no-deps to avoid torch conflicts)
RUN pip3 install --no-cache-dir --no-deps pyannote-audio==3.3.2

# torch_audiomentations: required by pyannote, but its dependency chain
# (torch_pitch_shift → torchaudio) would overwrite our shim.
# Install the chain with --no-deps and add leaf deps explicitly.
RUN pip3 install --no-cache-dir --no-deps torch_audiomentations torch_pitch_shift \
    && pip3 install --no-cache-dir julius primePy

# Remaining pyannote deps not covered by requirements.txt
# speechbrain: --no-deps to avoid pulling torchaudio; add its missing sub-deps
RUN pip3 install --no-cache-dir --no-deps speechbrain \
    && pip3 install --no-cache-dir sentencepiece hyperpyyaml semver tensorboardX matplotlib

# Install web stack dependencies (includes pyannote deps: pyannote-core,
# pyannote-pipeline, pytorch-metric-learning, asteroid-filterbanks, etc.)
COPY backend/requirements.txt /tmp/requirements.txt
RUN pip3 install --no-cache-dir \
    -r /tmp/requirements.txt \
    && rm /tmp/requirements.txt

# torchaudio shim: Pyannote needs torchaudio for audio I/O, but PyPI wheels
# have CUDA version mismatches with the torch we installed. Use our pure-Python
# shim (soundfile-based) that provides the subset pyannote actually uses.
# IMPORTANT: Must be installed AFTER all pip installs to prevent overwriting.
COPY backend/torchaudio_compat /tmp/torchaudio_shim
RUN pip3 uninstall -y torchaudio 2>/dev/null || true; \
    echo "Installing torchaudio shim (soundfile-based, no C++ ABI dependency)"; \
    SITE=$(python3 -c "import site; print(site.getsitepackages()[0])"); \
    rm -rf "$SITE/torchaudio"; \
    cp -r /tmp/torchaudio_shim "$SITE/torchaudio"; \
    python3 -c "import torchaudio; print(f'torchaudio shim OK')" \
    && rm -rf /tmp/torchaudio_shim

# spaCy English model for entity detection (CPU-only feature)
RUN python3 -m spacy download en_core_web_sm

# Copy backend code
WORKDIR /app
COPY backend/ /app/

# Copy built frontend
COPY --from=frontend-build /build/dist /app/static

# Copy entrypoint
COPY scripts/entrypoint-sherpa.sh /app/entrypoint-sherpa.sh
RUN chmod +x /app/entrypoint-sherpa.sh

ENV PORT=8001
ENV ENGINE=sherpa
ENV MODEL_ID=/models/sherpa-onnx

# Reduce ORT CUDA arena allocation — default kNextPowerOfTwo grossly over-allocates
ENV ORT_ARENA_EXTEND_STRATEGY=kSameAsRequested
EXPOSE 8001

ENTRYPOINT ["/app/entrypoint-sherpa.sh"]
